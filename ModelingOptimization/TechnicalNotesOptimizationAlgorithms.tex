%% LyX 2.2.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt,english]{article}
\usepackage{mathptmx}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{babel}
\usepackage[authoryear]{natbib}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=false]
 {hyperref}
\usepackage{breakurl}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Special footnote code from the package 'stblftnt.sty'
%% Author: Robin Fairbairns -- Last revised Dec 13 1996
\let\SF@@footnote\footnote
\def\footnote{\ifx\protect\@typeset@protect
    \expandafter\SF@@footnote
  \else
    \expandafter\SF@gobble@opt
  \fi
}
\expandafter\def\csname SF@gobble@opt \endcsname{\@ifnextchar[%]
  \SF@gobble@twobracket
  \@gobble
}
\edef\SF@gobble@opt{\noexpand\protect
  \expandafter\noexpand\csname SF@gobble@opt \endcsname}
\def\SF@gobble@twobracket[#1]#2{}

\makeatother

\begin{document}

\title{Notes on Optimization Algorithms}
\maketitle

\subsection*{Nelder-Mead Simplex Algorithm}

The algorithm iterates on the following steps:\footnote{For more details, see \url{http://www.mathworks.com/help/techdoc/math/bsotu2d.html\#bsgpq6p-11}}
\begin{enumerate}
\item Create a simplex of dimension $K+1$ (where there are $K$ parameters
to be estimated) around the starting value $x_{0}$.
\item Modify the simplex based on how the function looks at different trial
modifications. In particular, ``order the points in the simplex from
lowest function value $f(x(1))$ to highest $f(x(K+1))$. At each
step in the iteration, the algorithm discards the current worst point
$x(K+1)$, and accepts another point into the simplex'' (Matlab website).
Possible simplex modifications are:

\begin{enumerate}
\item Reflect
\item Expand
\item Contract outside
\item Contract inside
\item Shrink
\end{enumerate}
\item Repeat steps 1-2 until the diameter of the simplex is smaller than
the tolerance (default is $10^{-4}$)
\end{enumerate}

\subsection*{BFGS Medium-Scale algorithm: line search \textemdash{} No
user-supplied gradient or hessian}

The medium-scale algorithm iterates on the following four steps for
the $k$th iteration:
\begin{enumerate}
\item compute the search direction $p_{k}$ by solving
\begin{eqnarray*}
\hat{H}_{k}p_{k} & = & -\nabla f\left(x_{k}\right),\,\,\mbox{or}\\
p_{k} & = & -\hat{H}_{k}^{-1}\nabla f\left(x_{k}\right)
\end{eqnarray*}
where $\hat{H}_{k}$ is an approximation of the hessian.
\item Choose the step size $\alpha_{k}$ by solving
\[
\min_{\alpha}f\left(x_{k}+\alpha_{k}p_{k}\right)
\]
\item Update $x_{k+1}=x_{k}+\alpha_{k}p_{k}$
\item Update $\hat{H}_{k+1}$, letting $s_{k}=\alpha_{k}p_{k}$ and $y_{k}=\nabla f\left(x_{k+1}\right)-\nabla f\left(x_{k}\right)$:
\[
\hat{H}_{k+1}=\hat{H}_{k}+\frac{y_{k}y_{k}^{\prime}}{y_{k}^{\prime}s_{k}}-\frac{\hat{H}_{k}s_{k}s_{k}^{\prime}\hat{H}_{k}}{s_{k}^{\prime}\hat{H}_{k}s_{k}}
\]
where $^{\prime}$ refers to matrix transpose, not differentiation.
\item Continue steps 1-4 until $\left\Vert \nabla f\left(x_{k}\right)\right\Vert <\mbox{tolerance}$,
where $\mbox{tolerance}=10^{-6}$ by default.
\end{enumerate}

\subsection*{BFGS Large-Scale algorithm: trust region method \textemdash{} user-supplied
gradient and/or hessian}

The trust region method operates by dividing the optimization into
a series of 2-dimensional trust region subproblems. ``The basic idea
is to approximate $f$ with a simpler function $q$, which reasonably
reflects the behavior of function $f$ in a neighborhood $N$ around
the point $x$. This neighborhood is the \textbf{trust region}''
(Matlab website, emphasis added). The algorithm iterates on the following
steps:
\begin{enumerate}
\item Divide the problem into a series of 2-dimensional trust-region subproblems
\item Find the optimal step size $s$ by solving the following equation:
\[
\min_{s}\frac{1}{2}s^{\prime}Hs+s^{\prime}g\,\,\mbox{subject to}\,\,\left\Vert Ds\right\Vert <\Delta
\]
where $H$ is the hessian, $g$ is the gradient, $D$ is a diagonal
scaling matrix, and $\Delta$ is a positive scalar. $\left\Vert \cdot\right\Vert $
is the 2-norm.
\item If $f\left(x+s\right)<f\left(x\right)$ then $x_{k+1}=x_{k}+s$. If
not, then adjust $\Delta$.
\item Continue steps 1-3 until $\left\Vert \nabla f\left(x_{k}\right)\right\Vert <\mbox{tolerance}$,
or $f\left(x_{k+1}\right)-f\left(x_{k}\right)<\mbox{tolerance}$,
or $s<\mbox{tolerance}$(each $10^{-6}$ by default).
\end{enumerate}

\end{document}
